{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "src_dir = os.path.join(os.getcwd())\n",
    "abs_path = os.path.abspath(os.path.join(src_dir, os.pardir, 'src'))\n",
    "sys.path.append(abs_path)\n",
    "\n",
    "from utils import GLOBAL, functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nflfastpy as nfl\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change start and end for range\n",
    "pbp_df = pd.DataFrame()\n",
    "year_start = 2016\n",
    "year_end = 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\stala\\Desktop\\dev\\FFDataProject\\notebooks\\TDRegressionCandidates.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/stala/Desktop/dev/FFDataProject/notebooks/TDRegressionCandidates.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#Get roster based on last season, change date to last seasons\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/stala/Desktop/dev/FFDataProject/notebooks/TDRegressionCandidates.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m roster \u001b[39m=\u001b[39m nfl\u001b[39m.\u001b[39;49mload_roster_data(\u001b[39m2021\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\stala\\anaconda3\\envs\\FFDataProject\\lib\\site-packages\\nflfastpy\\__init__.py:44\u001b[0m, in \u001b[0;36mload_roster_data\u001b[1;34m(year)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mif\u001b[39;00m year \u001b[39m<\u001b[39m \u001b[39m1999\u001b[39m \u001b[39mor\u001b[39;00m year \u001b[39m>\u001b[39m \u001b[39m2021\u001b[39m:\n\u001b[0;32m     42\u001b[0m     \u001b[39mraise\u001b[39;00m SeasonNotFoundError(\u001b[39m'\u001b[39m\u001b[39mRoster data is only available from 1999 to 2021\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 44\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(ROSTER_URL\u001b[39m.\u001b[39;49mformat(year\u001b[39m=\u001b[39;49myear), low_memory\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     45\u001b[0m \u001b[39mreturn\u001b[39;00m df\n",
      "File \u001b[1;32mc:\\Users\\stala\\anaconda3\\envs\\FFDataProject\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\stala\\anaconda3\\envs\\FFDataProject\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\stala\\anaconda3\\envs\\FFDataProject\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\stala\\anaconda3\\envs\\FFDataProject\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:934\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 934\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\stala\\anaconda3\\envs\\FFDataProject\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1218\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1215\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1217\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1218\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1219\u001b[0m     f,\n\u001b[0;32m   1220\u001b[0m     mode,\n\u001b[0;32m   1221\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1222\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1223\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1224\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1225\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1226\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1227\u001b[0m )\n\u001b[0;32m   1228\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1229\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\stala\\anaconda3\\envs\\FFDataProject\\lib\\site-packages\\pandas\\io\\common.py:667\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    664\u001b[0m     codecs\u001b[39m.\u001b[39mlookup_error(errors)\n\u001b[0;32m    666\u001b[0m \u001b[39m# open URLs\u001b[39;00m\n\u001b[1;32m--> 667\u001b[0m ioargs \u001b[39m=\u001b[39m _get_filepath_or_buffer(\n\u001b[0;32m    668\u001b[0m     path_or_buf,\n\u001b[0;32m    669\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m    670\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m    671\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[0;32m    672\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m    673\u001b[0m )\n\u001b[0;32m    675\u001b[0m handle \u001b[39m=\u001b[39m ioargs\u001b[39m.\u001b[39mfilepath_or_buffer\n\u001b[0;32m    676\u001b[0m handles: \u001b[39mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[1;32mc:\\Users\\stala\\anaconda3\\envs\\FFDataProject\\lib\\site-packages\\pandas\\io\\common.py:336\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[39m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[0;32m    335\u001b[0m req_info \u001b[39m=\u001b[39m urllib\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[39m=\u001b[39mstorage_options)\n\u001b[1;32m--> 336\u001b[0m \u001b[39mwith\u001b[39;00m urlopen(req_info) \u001b[39mas\u001b[39;00m req:\n\u001b[0;32m    337\u001b[0m     content_encoding \u001b[39m=\u001b[39m req\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mContent-Encoding\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    338\u001b[0m     \u001b[39mif\u001b[39;00m content_encoding \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgzip\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    339\u001b[0m         \u001b[39m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\stala\\anaconda3\\envs\\FFDataProject\\lib\\site-packages\\pandas\\io\\common.py:236\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[39mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[39mthe stdlib.\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39murllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrequest\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m \u001b[39mreturn\u001b[39;00m urllib\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39murlopen(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\stala\\anaconda3\\envs\\FFDataProject\\lib\\urllib\\request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     opener \u001b[39m=\u001b[39m _opener\n\u001b[1;32m--> 216\u001b[0m \u001b[39mreturn\u001b[39;00m opener\u001b[39m.\u001b[39;49mopen(url, data, timeout)\n",
      "File \u001b[1;32mc:\\Users\\stala\\anaconda3\\envs\\FFDataProject\\lib\\urllib\\request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[39mfor\u001b[39;00m processor \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_response\u001b[39m.\u001b[39mget(protocol, []):\n\u001b[0;32m    524\u001b[0m     meth \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 525\u001b[0m     response \u001b[39m=\u001b[39m meth(req, response)\n\u001b[0;32m    527\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\stala\\anaconda3\\envs\\FFDataProject\\lib\\urllib\\request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \u001b[39m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m code \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m):\n\u001b[1;32m--> 634\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparent\u001b[39m.\u001b[39;49merror(\n\u001b[0;32m    635\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mhttp\u001b[39;49m\u001b[39m'\u001b[39;49m, request, response, code, msg, hdrs)\n\u001b[0;32m    637\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\stala\\anaconda3\\envs\\FFDataProject\\lib\\urllib\\request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[39mif\u001b[39;00m http_err:\n\u001b[0;32m    562\u001b[0m     args \u001b[39m=\u001b[39m (\u001b[39mdict\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhttp_error_default\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m orig_args\n\u001b[1;32m--> 563\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_chain(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[1;32mc:\\Users\\stala\\anaconda3\\envs\\FFDataProject\\lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[39mfor\u001b[39;00m handler \u001b[39min\u001b[39;00m handlers:\n\u001b[0;32m    495\u001b[0m     func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    497\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    498\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\stala\\anaconda3\\envs\\FFDataProject\\lib\\urllib\\request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhttp_error_default\u001b[39m(\u001b[39mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 643\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(req\u001b[39m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "#Get roster based on last season, change date to last seasons\n",
    "roster = nfl.load_roster_data(year_end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get yearly pbp data for past 5 years WARNING: Takes a long time\n",
    "for year in range(year_start, year_end):\n",
    "    yearly_df = nfl.load_pbp_data(year)\n",
    "    pbp_df = pd.concat([pbp_df, yearly_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter rushing data\n",
    "\n",
    "#rush_attempt = 1 if it was a rush_attempt, same with rush_touchdown, & two_point_attempt = 0 to filter out 2pt conversions\n",
    "rushing_df = pbp_df[['rush_attempt', 'rush_touchdown', 'yardline_100', 'two_point_attempt']]\n",
    "\n",
    "rushing_df = rushing_df.loc[\n",
    "    (rushing_df['two_point_attempt'] == 0) & (rushing_df['rush_attempt'] == 1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter receiving data\n",
    "\n",
    "#pass_attempt = 1 if it was a rush_attempt, same with pass_touchdown, & two_point_attempt = 0 to filter out 2pt conversions\n",
    "receiving_df = pbp_df[['pass_attempt', 'pass_touchdown', 'yardline_100', 'two_point_attempt']]\n",
    "\n",
    "receiving_df = receiving_df.loc[\n",
    "    (receiving_df['two_point_attempt'] == 0) & (receiving_df['pass_attempt'] == 1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, we are grouping by the yardline from where the play began, and then using value counts to count the number of times a rushing play was a touchdown (either a 0 or a 1), we can set the argument normalize = True to be able to calculate the proportion of plays that were touchdowns, instead of the count.\n",
    "\n",
    "rushing_df_probs = rushing_df.groupby('yardline_100')['rush_touchdown'].value_counts(normalize=True)\n",
    "\n",
    "rushing_df_probs = pd.DataFrame({\n",
    "    'probability_of_td': rushing_df_probs.values\n",
    "}, index=rushing_df_probs.index).reset_index()\n",
    "\n",
    "#Filter out prob of not a td\n",
    "rushing_df_probs = rushing_df_probs.loc[rushing_df_probs['rush_touchdown'] == 1]\n",
    "\n",
    "rushing_df_probs = rushing_df_probs.drop('rush_touchdown', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "receiving_df_probs = receiving_df.groupby('yardline_100')['pass_touchdown'].value_counts(normalize=True)\n",
    "\n",
    "receiving_df_probs = pd.DataFrame({\n",
    "    'probability_of_td': receiving_df_probs.values\n",
    "}, index=receiving_df_probs.index).reset_index()\n",
    "\n",
    "#Filter out prob of not a td\n",
    "receiving_df_probs = receiving_df_probs.loc[receiving_df_probs['pass_touchdown'] == 1]\n",
    "\n",
    "receiving_df_probs = receiving_df_probs.drop('pass_touchdown', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get PBP from the last season which is year_end variable\n",
    "last_season_pbp_df = nfl.load_pbp_data(year_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter out positions from last years rosters\n",
    "#RBs\n",
    "rb_df = roster.loc[roster['position'] == 'RB']['gsis_id']\n",
    "#WRs\n",
    "wr_df = roster.loc[roster['position'] == 'WR']['gsis_id']\n",
    "#TEs\n",
    "te_df = roster.loc[roster['position'] == 'TE']['gsis_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#RB TD Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get rushing data from last season pbp\n",
    "last_season_RB_rushing_df = last_season_pbp_df.loc[last_season_pbp_df['rush_attempt'] == 1, ['rusher_id', 'rusher_player_name', 'posteam', 'rush_attempt', 'rush_touchdown', 'yardline_100']]\n",
    "\n",
    "#Filter out RBs\n",
    "last_season_RB_rushing_df = last_season_RB_rushing_df.loc[last_season_RB_rushing_df['rusher_id'].isin(rb_df)]\n",
    "\n",
    "#Merge Probability df with Rushing\n",
    "last_season_RB_rushing_df = last_season_RB_rushing_df.merge(rushing_df_probs, how='left', on='yardline_100')\n",
    "\n",
    "#Calculate the actual touchdowns rb scored by aggregating all instances where the rush_touchdown == 1 & calculate aggregate of all instances where the probability of scoring a touchdown from that area of the field change names to better reflect what they represent\n",
    "last_season_RB_rushing_df = last_season_RB_rushing_df.groupby('rusher_id', as_index=False).agg({\n",
    "    'rusher_player_name': 'first',\n",
    "    'rush_touchdown': np.sum,\n",
    "    'probability_of_td': np.sum\n",
    "}).rename({\n",
    "    'probability_of_td': 'expected_touchdowns',\n",
    "    'rush_touchdown': 'actual_touchdowns'\n",
    "}, axis=1)\n",
    "\n",
    "#Determine if positive regression candidate by comparing actual scored touchdowns vs expected touchdowns\n",
    "last_season_RB_rushing_df['positive_regression_candidate'] = last_season_RB_rushing_df['actual_touchdowns'] < last_season_RB_rushing_df['expected_touchdowns']\n",
    "\n",
    "#Get delta between expected vs actual tds\n",
    "last_season_RB_rushing_df['delta'] = last_season_RB_rushing_df.apply(\n",
    "    lambda x: abs(x.expected_touchdowns - x.actual_touchdowns), axis=1\n",
    ")\n",
    "\n",
    "last_season_RB_rushing_df = last_season_RB_rushing_df.sort_values(by='expected_touchdowns', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#WR TD Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get receiving data from last season pbp\n",
    "last_season_WR_receiving_df = last_season_pbp_df.loc[last_season_pbp_df['pass_attempt'] == 1, ['receiver_id', 'receiver_player_name', 'posteam', 'pass_attempt', 'pass_touchdown', 'yardline_100']]\n",
    "\n",
    "#Filter out WRs\n",
    "last_season_WR_receiving_df = last_season_WR_receiving_df.loc[last_season_WR_receiving_df['receiver_id'].isin(wr_df)]\n",
    "\n",
    "#Merge Probability df with receiving\n",
    "last_season_WR_receiving_df = last_season_WR_receiving_df.merge(receiving_df_probs, how='left', on='yardline_100')\n",
    "\n",
    "#Calculate the actual touchdowns rb scored by aggregating all instances where the rush_touchdown == 1 & calculate aggregate of all instances where the probability of scoring a touchdown from that area of the field change names to better reflect what they represent\n",
    "last_season_WR_receiving_df = last_season_WR_receiving_df.groupby('receiver_id', as_index=False).agg({\n",
    "    'receiver_player_name': 'first',\n",
    "    'pass_touchdown': np.sum,\n",
    "    'probability_of_td': np.sum\n",
    "}).rename({\n",
    "    'probability_of_td': 'expected_touchdowns',\n",
    "    'pass_touchdown': 'actual_touchdowns'\n",
    "}, axis=1)\n",
    "\n",
    "#Determine if positive regression candidate by comparing actual scored touchdowns vs expected touchdowns\n",
    "last_season_WR_receiving_df['positive_regression_candidate'] = last_season_WR_receiving_df['actual_touchdowns'] < last_season_WR_receiving_df['expected_touchdowns']\n",
    "\n",
    "#Get delta between expected vs actual tds\n",
    "last_season_WR_receiving_df['delta'] = last_season_WR_receiving_df.apply(\n",
    "    lambda x: abs(x.expected_touchdowns - x.actual_touchdowns), axis=1\n",
    ")\n",
    "\n",
    "last_season_WR_receiving_df = last_season_WR_receiving_df.sort_values(by='expected_touchdowns', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TE TD Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get receiving data from last season pbp\n",
    "last_season_TE_receiving_df = last_season_pbp_df.loc[last_season_pbp_df['pass_attempt'] == 1, ['receiver_id', 'receiver_player_name', 'posteam', 'pass_attempt', 'pass_touchdown', 'yardline_100']]\n",
    "\n",
    "#Filter out WRs\n",
    "last_season_TE_receiving_df = last_season_TE_receiving_df.loc[last_season_TE_receiving_df['receiver_id'].isin(te_df)]\n",
    "\n",
    "#Merge Probability df with receiving\n",
    "last_season_TE_receiving_df = last_season_TE_receiving_df.merge(receiving_df_probs, how='left', on='yardline_100')\n",
    "\n",
    "#Calculate the actual touchdowns rb scored by aggregating all instances where the rush_touchdown == 1 & calculate aggregate of all instances where the probability of scoring a touchdown from that area of the field change names to better reflect what they represent\n",
    "last_season_TE_receiving_df = last_season_TE_receiving_df.groupby('receiver_id', as_index=False).agg({\n",
    "    'receiver_player_name': 'first',\n",
    "    'pass_touchdown': np.sum,\n",
    "    'probability_of_td': np.sum\n",
    "}).rename({\n",
    "    'probability_of_td': 'expected_touchdowns',\n",
    "    'pass_touchdown': 'actual_touchdowns'\n",
    "}, axis=1)\n",
    "\n",
    "#Determine if positive regression candidate by comparing actual scored touchdowns vs expected touchdowns\n",
    "last_season_TE_receiving_df['positive_regression_candidate'] = last_season_TE_receiving_df['actual_touchdowns'] < last_season_TE_receiving_df['expected_touchdowns']\n",
    "\n",
    "#Get delta between expected vs actual tds\n",
    "last_season_TE_receiving_df['delta'] = last_season_TE_receiving_df.apply(\n",
    "    lambda x: abs(x.expected_touchdowns - x.actual_touchdowns), axis=1\n",
    ")\n",
    "\n",
    "last_season_TE_receiving_df = last_season_TE_receiving_df.sort_values(by='expected_touchdowns', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('FFDataProject')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c8217a67ec537b0e88bf5d3449b6851d0e6559ef2b1251532e5b344fa3f1273"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
